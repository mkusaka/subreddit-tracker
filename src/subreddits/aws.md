# aws
## [1][PSA: Don't take remote exams offered by Pearson Vue (OnVue) for AWS Certifications!](https://www.reddit.com/r/aws/comments/fscq7v/psa_dont_take_remote_exams_offered_by_pearson_vue/)
- url: https://www.reddit.com/r/aws/comments/fscq7v/psa_dont_take_remote_exams_offered_by_pearson_vue/
---
I can't describe how horrible this experience was.  I am not looking forward to how much work I am going to have to do to get my money back.  This is not my first AWS certification (I have SA Pro and DevOps Pro), but is my first online exam.  The short version is: Don't take AWS exams via the Pearson Vue at home option, even if it is offered.  AWS should not be offering this option as I can attest it is a waste of time.  Ironically, AWS would have us use their services because of their high availability and scaling but apparently they don't ask their test partners to do the same!

It started off easy enough: I passed the initial 'checks' as it confirmed my internet speed, camera access, and microphone access.  I started the process 15+ minutes before my scheduled exam time.  I was able to open the app, it again verified the technical requirements passed, and I went to the next screen.  It asked for my cell phone number and texted me a link which opened a web page which requested to take my photo.  Easy enough.  I did that and then the web page went to 'Uploading and verifying photo'.  A spinning circle started spinning.  This is where my test experience ended, but not where the poor experience ends.  I tried again, and then a third time.  Same experience.  As I write this, I left it on that page and the spinning is continuing.  This screen has been spinning for no less than 45 minutes.  At 8 minutes before my scheduled exam, I tried finding the help link.  A chat window opened, and I waited, and waited, and waited.  Still waiting as I write this.  My chat window has been open for 52 minutes and still no one to help.  Every two minutes I get ' All agents are currently assisting others. Thank you for your patience.' written in the window.  OK - what next?  They make it harder to find, but I got a phone number I can call.  I tried calling that.  Busy signal.  For the next 20 minutes I called back and back, busy signal.  Finally, I got it to actually pick up, but of course no human yet.  No estimate of time to when I can be helped.  They don't even have nice elevator music to listen to.  Who knows when I will be able to talk to someone.  This has been an exceedingly poor experience.

If you value your time, please do yourself a favor and don't even attempt a online exam with Pearson.  I worked hard to prepare for this exam and rescheduled things to fit around it.  Now, I will have to do that all again.

u/jeffbarr Is this the experience AWS is hoping to get with their testing partners?  This was a waste of my time and money.  Amazon should seriously reevaluate the quality of their test partners.  I understand everyone is trying to deal with all the issues.  However, if you can't offer quality testing, then please don't offer the option at all.  It isn't respectful to people's time.  Pearson is well aware of their capacity and if it isn't up to requirements, they shouldn't be scheduling test slots.

&amp;#x200B;

*EDIT*: A few background items I didn't initially share that may be relevant for others.  For the computer, I used a fully up to date Windows 10 laptop.  The laptop itself is only about a month old and is in near pristine condition.  Other than a few applications like Office, there is barely anything installed on there yet.  I used a hard wired connection, like recommended by Pearson through the use of a usb-to-ethernet adapter.  I have Verizon FIOS (980Mbps/840Mbps) and did do a speed test way after it was apparent this would not work.  I forget the exact numbers, but I was still pulling in hundreds of Mbps in both directions, despite everyone being at home and using the USB ethernet adapater which does put a cap on my speed, but I can't see hundreds of Mbps not being sufficent by orders of magnatude.  My phone is a fully up to date pixel 3.  I tried using my wifi in my house first (connected through FIOS), and then using the phone 4G LTE connection.  I can't imagine this was caused by my end.  It seemed like Pearson's servers were jammed at that point in time.

&amp;#x200B;

*Update*: After a LONG time, I did eventually get someone to answer from Pearson.  They were nice enough and were fairly easy to understand, although there was an delay echo introduced where whatever I said was echoed a quarter to half second later which was annoying, but bearable.  I was just happy she was able to hear me.  She said she could open a trouble ticket for me, but as it was well over an hour trying to get through to any human and doubtful it was on my side, I just told her to schedule me for the next available in person appointment.  She had to cancel my appointment and then rebook it as their sub-standard system wouldn't let her reschedule an at home appointment to at a location.  Surprisingly, she said they would refund my money and rebook me.  It was painless enough, but when I asked for a reference number on the refund, all she could do is say I 'should' get an email.  Perhaps unsurprisingly, this morning I see a fully posted charge for the rescheduled exam, but no sign of a refund.  Sigh.  I will give it a few days and then start this process over.

For what its worth, people should IGNORE the advice that the web chat is the fastest way of getting help.  Find the phone number and dial and re-dial it as fast as you can when you get a busy signal.  Despite the fact that it took 20+ minutes to get the number to pickup (and was 'waiting' 20 minutes less from the phones point of view) I got a faster response from someone on the phone.  Web based chat never picked up, even though I left it running during my entire phone conversation.

*Update #2*: It took two more days than the charge, but the refund did show up in the correct amount on my credit card.  I am actually quite surprised.
## [2][I wrote a guide on deploying a Lambda/API Gateway API with AWS Serverless Application Model (SAM)](https://www.reddit.com/r/aws/comments/hlshss/i_wrote_a_guide_on_deploying_a_lambdaapi_gateway/)
- url: https://seanjziegler.com/how-to-build-an-api-with-aws-lambda-and-api-gateway-using-aws-sam/
---

## [3][A Cost Calculator Project](https://www.reddit.com/r/aws/comments/hm23kn/a_cost_calculator_project/)
- url: https://www.reddit.com/r/aws/comments/hm23kn/a_cost_calculator_project/
---
Hello guys,

I have created a very basic Cloud Cost Calculator for AWS Services. This project was intended to learn angular and how to use AWS API with that. Please take a look at it [here](https://github.com/hetjagani/CloudCostCalculator) and suggest ideas if any. This project would be a good start if anyone is learning angular development or using AWS SDK for javascript. Feel free to extend the project and submit pull requests.

&amp;#x200B;

[Demo](https://i.redd.it/10lus6tpg6951.gif)
## [4][Possible to convert EC2 reserved instance to RDS?](https://www.reddit.com/r/aws/comments/hm7gns/possible_to_convert_ec2_reserved_instance_to_rds/)
- url: https://www.reddit.com/r/aws/comments/hm7gns/possible_to_convert_ec2_reserved_instance_to_rds/
---
We recently moved a SQL server from an EC2 r5 large to an RDS r5 large.  I thought the reserved instance we had on EC2 would apply to RDS but it seems that is not the case.  Is there a way to transfer it?
## [5][HTTP API: OAuth scopes for user-level ACL](https://www.reddit.com/r/aws/comments/hm7214/http_api_oauth_scopes_for_userlevel_acl/)
- url: https://www.reddit.com/r/aws/comments/hm7214/http_api_oauth_scopes_for_userlevel_acl/
---
Hi there,

I'm currently developing a web application using an HTTP API from API Gateway and Cognito.

As described in [the docs](https://aws.amazon.com/premiumsupport/knowledge-center/cognito-custom-scopes-api-gateway/?nc1=h_ls), I'd like to use OAuth scopes for user-level access-control management. More precisely, I would like to set the users' roles (admin, superadmin etc...) in their JWTs "scope" claim, and have API Gateway compare them to the scopes of the ressource they're trying to access, allowing early deny if both don't match.

It seems like the OAuth scopes are usually reserved for third-party apps access-control, and that the only way to do that would be to store the roles in a db, and adding them manually with a pre-token generation lambda.

This solution seems a bit hacky, but much easier to set-up and use compared to placing users in Cognito groups with given IAM roles.

[ACL with scopes](https://preview.redd.it/omt7721fh8951.jpg?width=929&amp;format=pjpg&amp;auto=webp&amp;s=755d521135ddd6ae23ce9cf2ee00590d0b00a824)

Do you think it a good idea ? Is there any impact I didn't see that would mean a no-go ?
## [6][Choosing RDS/Aurora instance](https://www.reddit.com/r/aws/comments/hm360q/choosing_rdsaurora_instance/)
- url: https://www.reddit.com/r/aws/comments/hm360q/choosing_rdsaurora_instance/
---
I have a 7Gb MySQL database in the cloud which is only used by 8 users (and this user base will not increase significantly, since it is not for an application, but for own usage for research purposes). We are currently being billed around 250$/month, which sounded like too much for just 7Gb to me.  


I did some research and found out that RDS provide up to 64 Tb of database, and the price tag seems to be more related with the CPU power than with the size of the database.  


Hence, the only problem now is choosing the correct instance. A T3-XL instance costs aprox. the same amount we are currently paying, while T3-L provides around 60% of savings, and a T3-M about 80% of savings. Given the small amount of users, even with concurrent usage, we might be able to go even for a lower instance, or even T2 or M5 (I don't know which one is better, but again I feel that our needs are not too high when compared with a massive userbase web app).  


What kind of instance would you recommend? Is there an easy way of migrating to a larger instance later on if neccessary? Can we allocate more memory when our database increase, or do I need to pay for, for example, 50 Gb from day 1 even if we are only using 7Gb at the moment?  


Thanks for your time
## [7][access key id and secret key id not working](https://www.reddit.com/r/aws/comments/hm5lqi/access_key_id_and_secret_key_id_not_working/)
- url: https://www.reddit.com/r/aws/comments/hm5lqi/access_key_id_and_secret_key_id_not_working/
---
I am trying to use amazon web hosting for my new website but when i generate access key id and secret key id it shows it is active on AWS site but when i am trying to use this keys in word press to create a new cloud it is not working and this message pop up on screen

https://preview.redd.it/bjcpbd2yx7951.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=c7bbcd269cf9ce2580a8b912e608807435591549
## [8][aws-cli question](https://www.reddit.com/r/aws/comments/hm5b2h/awscli_question/)
- url: https://www.reddit.com/r/aws/comments/hm5b2h/awscli_question/
---
Hello All,

I have an aws-cli question. I will admit I'm using wasabi instead of the official aws s3 services, but they claim to be fully compatible. when I run the aws s3 sync command, I am able to sync and upload everything in the folder I'm trying to sync. It is a folder with only a few GB in it and so it sync's fairly quickly, however, if I wait a minute and run it again it actually uploads a significant portion AGAIN! I have run the command several times back-to-back and it keeps uploading. I see that the modified date/time is accurate to the upload, but I can't seem to figure out why? when none of the known attributes have changed.

&amp;#x200B;

any thoughts?
## [9][Periodically Trigger AWS Lambda to Download CSV Files Off a Server and Upload to S3](https://www.reddit.com/r/aws/comments/hm0kwg/periodically_trigger_aws_lambda_to_download_csv/)
- url: https://www.reddit.com/r/aws/comments/hm0kwg/periodically_trigger_aws_lambda_to_download_csv/
---
New to AWS, I was wondering if it's possible to create a lambda that would periodically download (say, every hour) CSV files from a secure server which obeys SFTP protocol. This would then be uploaded into a S3 bucket automatically for further analysis. 

Currently the process is being done via FileZilla transfer, which is why I'm trying to automate this. I'm trying to look for documentation and videos online but haven't found any good examples yet. How would be the best way to go about this?
## [10][Amazon Lex proactively asking questions](https://www.reddit.com/r/aws/comments/hm4bg6/amazon_lex_proactively_asking_questions/)
- url: https://www.reddit.com/r/aws/comments/hm4bg6/amazon_lex_proactively_asking_questions/
---
Hey there!

I want to build a „virtual trainer“ bot that actively asks a series of (more or less predefined) questions to a specific topic, collects responses and provides feedback on the given responses. All of that in a conversational manner (as far as that is possible). 

However, from what I have seen, the problem with Lex is that it is mostly designed as a Question -&gt; Answer system, that responds to a user‘s intent and gives responses based on that information. However, it can not directly initiate questions itself without any hacky workarounds.

Any ideas / thoughts on how this could possibly work?

Thanks!
## [11][Overtly complex bash script to create an encrypted AMI: is there a better approach available?](https://www.reddit.com/r/aws/comments/hm3xd8/overtly_complex_bash_script_to_create_an/)
- url: https://www.reddit.com/r/aws/comments/hm3xd8/overtly_complex_bash_script_to_create_an/
---
### Background

I have two accounts:

* Account "1111" where our organization stores its custom baked AMIs in
* Account "2222" that is dedicated to a new app/service we are building

The `2222` account has acccess to AMIs owned by `1111`.

We also have a custom API that allows us to query for the latest patched AMI we should use for building new EC2 instances:

    https://ami.example.com

We can send a request to this endpoint to get our latest RHEL (RedHat Enterprise Linux) AMI:

    curl https://ami.example.com?distro=rhel&amp;distroVersion=7&amp;amiVersion=latest

This will return a response like this:

    ami-1234

Whose owner is the `1111` account.

### Problem

Now instead of creating the EC2 instance using the `ami-1234` AMI, there's a script that performs the following activities:

    #!/bin/bash

    # This returns something like ami-1234, an AMI owned by the 1111 account
    source_image_id=$(curl -s https://ami.example.com?distro=rhel&amp;distroVersion=7&amp;amiVersion=latest)

    # This returns something like snap-1415
    source_snapshot_id=$(aws ec2 describe-images --region ... -- image_id ${image_id} --query "Images[*].BlockDeviceMappings[*].Ebs.SnapshotId" --output text)

    # This returns something like source-rhel-7-20200701
    source_ami_name=$(aws ec2 describe-images --region ... -- image_id ${image_id} --query "Images[*].Name" --output text)

    # This returns something like AppName-source-rhel-7-20200701
    target_ami_name=$(aws ec2 describe-images --region ... --query "Images[*].Name" --filters "Name=owner-id,Values=2222" | sort ... | tail ... | awk ...)

    if [ "$target_ami_name" == "AppName-${source_ami_name}-encrypted" ]; then
        // AMI is up to date, fetch the AMI ID
        ami_id=$(aws ec2 describe-images --region ... --query "Images[*].ImageId" --filters "Name=owner-id,Values=2222,Name=name,Values=${target_ami_name}" --output text)
    else
        encrypted_snapshot_id=$(aws ec2 copy-snapshot --region ... --source-region ... --source-snapshot-id ${source_snapshot_id} --encrypted --kms-key-id "alias/AppNameKmsKey" -- description "Encrypted AppName snapshot created from ${source_snapshot_id}")
        
        # Wait for the snapshot to become ready
        status=pending
        whle ["${status}" != "completed"]; do
            sleep 10
            status=$(aws ec2 describe-snapshots --snapshot-ids ${encrypted_snapshot_id} --query "Snapshots[*].State" --output text)
        done
    end

    ami_id=$(aws ec2 register-image --region ... --name "AppName-${source_ami_name}-encrypted" --description ... --virtualization-type hvm --architecture x86_64 --root-device-name /dev/sda1 \
             --block-device-mappings '[{"DeviceName" : "/dev/sda1", "Ebs" : {"SnapshotId" : "encrypted_snapshot_id"}, "VolumeType": "gp2", "VolumeSize" : 60}]' --output text)


    echo "${ami_id}"

As far as I can tell this script:

* Copies the `source_snapshot_id` snapshot from the `1111` account to the `2222` account,
* Encrypyts the new snapshot using a KMS key owned by the `2222` account (i.e. `encrypted_snapshot_id`)
* Creates an EBS backed AMI using the `encrypted_snapshot_id` snapshot

Then the output of this script (the new AMI's ID) is passed to various CloudFormation templates to be used as the image for EC2 instances created in new stacks.

Browsing through our organization's requirements for EC2 instances, I can see there's a requirement for certain apps to use encrypted EBS volumes. But isn't there a more elegant method to achieve this, ideally with the logic inside CloudFormation templates?
