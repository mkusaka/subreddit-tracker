# aws
## [1][AWS Wish List 2020](https://www.reddit.com/r/aws/comments/jbw85e/aws_wish_list_2020/)
- url: https://www.reddit.com/r/aws/comments/jbw85e/aws_wish_list_2020/
---
&amp;#x200B;

AWS always releases a bunch of features, sometimes everyday or atleast once a week. Here is my wish list of the features I want to see as a part of AWS infrastructure

1: AWS Managed Proxy Server(Rather than spinning own squid server)

2: EBS replication across different availability zones(Possible? Legal constraints?)

3: Multi-region VPC(Possible? Legal constraints?)

4: UI to debug boot issues(Better then EC2 Get Instance Screenshot and Instance logs)

5: Support tagging for every individual service(It's improving)

6: VPC endpoints support for every service (EKS?) 

7: EC2 instance live migration

8: Display AWS Cli  while resource creation(Similar to GCP)

9: Cost calculation while resource creation(AWS start supporting(for example, RDS) this feature but not for every service

10: More features in App Mesh(Circuit breaker, Rate Limiting)

P.S: Not sure if some features are already available, but if something is missing, please feel free to add
## [2][[QUESTION] Best course/way to understand dynamoDB modeling and how to structure your tables keys etc?](https://www.reddit.com/r/aws/comments/jgzave/question_best_courseway_to_understand_dynamodb/)
- url: https://www.reddit.com/r/aws/comments/jgzave/question_best_courseway_to_understand_dynamodb/
---
Do you guys know of any course or tutorials that explain data structures in dynamo db using real life scenarios? I watched every video in youtube but still I dont quite feel like I understand it?
## [3][Introducing the AWS Load Balancer Controller](https://www.reddit.com/r/aws/comments/jh07g7/introducing_the_aws_load_balancer_controller/)
- url: https://aws.amazon.com/about-aws/whats-new/2020/10/introducing-aws-load-balancer-controller/
---

## [4][Infra App (v0.42) - Simplest Kubernetes desktop client now supports all resources](https://www.reddit.com/r/aws/comments/jh3447/infra_app_v042_simplest_kubernetes_desktop_client/)
- url: https://www.reddit.com/r/aws/comments/jh3447/infra_app_v042_simplest_kubernetes_desktop_client/
---
Hey everyone!

Thanks to all your feedback and supporting us, we've made a big release this week to include support for all Kubernetes resources in Infra App. Now you can use Infra App to check on a secret or see a configuration of a CRD, and much more.

More on the update here: [https://infra.app/blog/v0-42-biggest-update-yet-all-resources-now-available](https://infra.app/blog/v0-42-biggest-update-yet-all-resources-now-available)

For others who haven't heard about Infra App, my co-founder and I have been building this app with the goal of creating a simple and easy to use Kubernetes desktop client. Some of the notable features are

* Live tailing of logs / aggregating related logs for you so you can filter and search them
* CPU / Memory metrics directly from your cluster (provided you have metrics-server)
* Displaying the relationship of your related sources
* All this is local to your computer, we don't send your cluster information to us (it's the reason why we're building this as a desktop app)

You can find more about Infra App at [https://infra.app](https://infra.app/)

We're just super passionate about this space and want to create a great sustainable app.

It's available on Windows / Mac / Linux.

&amp;#x200B;

https://preview.redd.it/t66c6soj4zu51.png?width=3072&amp;format=png&amp;auto=webp&amp;s=c70cae8e4c3bf39815b324b22c3427fe6252d276

https://preview.redd.it/fpzpl6mq4zu51.png?width=1387&amp;format=png&amp;auto=webp&amp;s=74ae03ab9d8bb500fecbce2cefcdda20a8cbfb7f

https://preview.redd.it/3ypux0wl4zu51.png?width=1387&amp;format=png&amp;auto=webp&amp;s=b1ea5c13c3faba321fd9ad21cb3d10f9f69eed0c
## [5][Did speed test for transfer acceleration... if something is “Infinity% Faster,” does it already exist before I transfer? Does the transfer happen backwards in time?](https://www.reddit.com/r/aws/comments/jgs11a/did_speed_test_for_transfer_acceleration_if/)
- url: https://i.redd.it/l7c7d52utvu51.jpg
---

## [6][How to join AWS slack community](https://www.reddit.com/r/aws/comments/jh6195/how_to_join_aws_slack_community/)
- url: https://www.reddit.com/r/aws/comments/jh6195/how_to_join_aws_slack_community/
---
Hi, i am not able join 

[https://og-aws.slack.com/#](https://og-aws.slack.com/#)

[https://awsdevelopers.slack.com/#](https://awsdevelopers.slack.com/#)

these 2 slack channel, if anyone know how to join please helm.

Thanks
## [7][CloudFormation Team Public Roadmap](https://www.reddit.com/r/aws/comments/jgxmx0/cloudformation_team_public_roadmap/)
- url: https://www.reddit.com/r/aws/comments/jgxmx0/cloudformation_team_public_roadmap/
---
Disclaimer: I work at AWS. But nowhere near the CloudFormation service team. Most of our visibility on upcoming features of CFN come from this roadmap. 

I was reminded of this when I saw the announcement about the service limit increase. 



https://github.com/aws-cloudformation/aws-cloudformation-coverage-roadmap
## [8][Should public and private objects placed in same or different S3 bucket?](https://www.reddit.com/r/aws/comments/jh5zsa/should_public_and_private_objects_placed_in_same/)
- url: https://www.reddit.com/r/aws/comments/jh5zsa/should_public_and_private_objects_placed_in_same/
---
I’m working on a SaaS application. Currently we’re only storing public uploads in a bucket. For a new feature I need to store private files only accessible for specific users.

What would you say is best practice, having a separate bucket for all private objects or just one bucket with only specific folders/objects that are public?
## [9][AWS CloudFormation now supports increased limits on five service quotas](https://www.reddit.com/r/aws/comments/jgnlbh/aws_cloudformation_now_supports_increased_limits/)
- url: https://www.reddit.com/r/aws/comments/jgnlbh/aws_cloudformation_now_supports_increased_limits/
---
https://aws.amazon.com/about-aws/whats-new/2020/10/aws-cloudformation-now-supports-increased-limits-on-five-service-quotas/
## [10][Dumb S3 newbie, learning my lesson about Rclone &amp; GDA Early Deletion Fees](https://www.reddit.com/r/aws/comments/jh0g3q/dumb_s3_newbie_learning_my_lesson_about_rclone/)
- url: https://www.reddit.com/r/aws/comments/jh0g3q/dumb_s3_newbie_learning_my_lesson_about_rclone/
---
Apologies in advance for the dumb questions, but I have a video production company that wants to use Glacier Deep Archive as a replacement for our redundant offsite LTO backups of raw camera footage.

I used Rclone to upload a 4 TB archive directly to GDA, which was all fine and dandy.

This was the command line I used:

`rclone sync --verbose --progress --s3-upload-concurrency 16 --s3-chunk-size 16M --s3-disable-checksum --log-file=rclone.log --exclude '.*' --bwlimit "07:30,13M 18:00,off" /ArchiveDrive/Footage remote:example.bucket`

Well, as it turns out, when I tried to add additional files to the bucket (from another archive drive) using "rclone sync", it ended up deleting almost everything I had already uploaded to GDA, because I was too stupid to not realize that I should have been using "rclone *copy*" instead of "rclone sync"!

As a result, I am being charged $50 a month for the next 90 days for the accidental early deletion (sadface)

Now that I've learned my lesson about rclone, I would like to confirm with all the S3 veterans here this would be a better/safer/correct way to do this:

1. Change my rclone config to use ONEZONE\_IA the default storage class, instead of DEEP\_ARCHIVE
2. Use "rclone copy" to upload my archives to my S3 bucket, with OneZone-IA as the storage class
3. Set a Lifecycle policy on that bucket to move everything to GDA after 1 day

Am I correct that uploading to OneZone-IA would prevent me being charged for early deletion fees in the event that I make a mistake with Rclone, while minimizing as much cost as possible for daily storage of the archive before it gets moved to GDA?

Seems to me that uploading directly to Glacier or GDA is a "don't try this at home" option best left to the experts?
## [11][Question: How access server files via webbrowser?](https://www.reddit.com/r/aws/comments/jh78hf/question_how_access_server_files_via_webbrowser/)
- url: https://www.reddit.com/r/aws/comments/jh78hf/question_how_access_server_files_via_webbrowser/
---
Hi There,

&amp;#x200B;

I have an EC2 instance, and I have uploaded some files into it (script).

I must now access these files, to be exactly install.php, via webbrowser, to install the script.

\[ex: publicdns/home/install.php\]

&amp;#x200B;

As it comes, I only have a public DNS adress (or public IP), no domain name.

&amp;#x200B;

When I type in the public DNS adress / or IP in my webbrowser, it just throws me the message:

Not able to connect, etc.

&amp;#x200B;

&amp;#x200B;

So, what can I do here to access my files via webbrowser and install my script?!

&amp;#x200B;

&amp;#x200B;

Thanks
