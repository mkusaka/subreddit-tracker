{"kind": "Listing", "data": {"modhash": "", "dist": 11, "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "aws", "selftext": "I can't describe how horrible this experience was.  I am not looking forward to how much work I am going to have to do to get my money back.  This is not my first AWS certification (I have SA Pro and DevOps Pro), but is my first online exam.  The short version is: Don't take AWS exams via the Pearson Vue at home option, even if it is offered.  AWS should not be offering this option as I can attest it is a waste of time.  Ironically, AWS would have us use their services because of their high availability and scaling but apparently they don't ask their test partners to do the same!\n\nIt started off easy enough: I passed the initial 'checks' as it confirmed my internet speed, camera access, and microphone access.  I started the process 15+ minutes before my scheduled exam time.  I was able to open the app, it again verified the technical requirements passed, and I went to the next screen.  It asked for my cell phone number and texted me a link which opened a web page which requested to take my photo.  Easy enough.  I did that and then the web page went to 'Uploading and verifying photo'.  A spinning circle started spinning.  This is where my test experience ended, but not where the poor experience ends.  I tried again, and then a third time.  Same experience.  As I write this, I left it on that page and the spinning is continuing.  This screen has been spinning for no less than 45 minutes.  At 8 minutes before my scheduled exam, I tried finding the help link.  A chat window opened, and I waited, and waited, and waited.  Still waiting as I write this.  My chat window has been open for 52 minutes and still no one to help.  Every two minutes I get ' All agents are currently assisting others. Thank you for your patience.' written in the window.  OK - what next?  They make it harder to find, but I got a phone number I can call.  I tried calling that.  Busy signal.  For the next 20 minutes I called back and back, busy signal.  Finally, I got it to actually pick up, but of course no human yet.  No estimate of time to when I can be helped.  They don't even have nice elevator music to listen to.  Who knows when I will be able to talk to someone.  This has been an exceedingly poor experience.\n\nIf you value your time, please do yourself a favor and don't even attempt a online exam with Pearson.  I worked hard to prepare for this exam and rescheduled things to fit around it.  Now, I will have to do that all again.\n\nu/jeffbarr Is this the experience AWS is hoping to get with their testing partners?  This was a waste of my time and money.  Amazon should seriously reevaluate the quality of their test partners.  I understand everyone is trying to deal with all the issues.  However, if you can't offer quality testing, then please don't offer the option at all.  It isn't respectful to people's time.  Pearson is well aware of their capacity and if it isn't up to requirements, they shouldn't be scheduling test slots.\n\n&amp;#x200B;\n\n*EDIT*: A few background items I didn't initially share that may be relevant for others.  For the computer, I used a fully up to date Windows 10 laptop.  The laptop itself is only about a month old and is in near pristine condition.  Other than a few applications like Office, there is barely anything installed on there yet.  I used a hard wired connection, like recommended by Pearson through the use of a usb-to-ethernet adapter.  I have Verizon FIOS (980Mbps/840Mbps) and did do a speed test way after it was apparent this would not work.  I forget the exact numbers, but I was still pulling in hundreds of Mbps in both directions, despite everyone being at home and using the USB ethernet adapater which does put a cap on my speed, but I can't see hundreds of Mbps not being sufficent by orders of magnatude.  My phone is a fully up to date pixel 3.  I tried using my wifi in my house first (connected through FIOS), and then using the phone 4G LTE connection.  I can't imagine this was caused by my end.  It seemed like Pearson's servers were jammed at that point in time.\n\n&amp;#x200B;\n\n*Update*: After a LONG time, I did eventually get someone to answer from Pearson.  They were nice enough and were fairly easy to understand, although there was an delay echo introduced where whatever I said was echoed a quarter to half second later which was annoying, but bearable.  I was just happy she was able to hear me.  She said she could open a trouble ticket for me, but as it was well over an hour trying to get through to any human and doubtful it was on my side, I just told her to schedule me for the next available in person appointment.  She had to cancel my appointment and then rebook it as their sub-standard system wouldn't let her reschedule an at home appointment to at a location.  Surprisingly, she said they would refund my money and rebook me.  It was painless enough, but when I asked for a reference number on the refund, all she could do is say I 'should' get an email.  Perhaps unsurprisingly, this morning I see a fully posted charge for the rescheduled exam, but no sign of a refund.  Sigh.  I will give it a few days and then start this process over.\n\nFor what its worth, people should IGNORE the advice that the web chat is the fastest way of getting help.  Find the phone number and dial and re-dial it as fast as you can when you get a busy signal.  Despite the fact that it took 20+ minutes to get the number to pickup (and was 'waiting' 20 minutes less from the phones point of view) I got a faster response from someone on the phone.  Web based chat never picked up, even though I left it running during my entire phone conversation.\n\n*Update #2*: It took two more days than the charge, but the refund did show up in the correct amount on my credit card.  I am actually quite surprised.", "author_fullname": "t2_43vca68k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: Don't take remote exams offered by Pearson Vue (OnVue) for AWS Certifications!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/aws", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_fscq7v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 98, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "training/certification", "can_mod_post": false, "score": 98, "approved_by": null, "author_premium": false, "thumbnail": "", "edited": 1585824763.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1585689396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.aws", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I can&amp;#39;t describe how horrible this experience was.  I am not looking forward to how much work I am going to have to do to get my money back.  This is not my first AWS certification (I have SA Pro and DevOps Pro), but is my first online exam.  The short version is: Don&amp;#39;t take AWS exams via the Pearson Vue at home option, even if it is offered.  AWS should not be offering this option as I can attest it is a waste of time.  Ironically, AWS would have us use their services because of their high availability and scaling but apparently they don&amp;#39;t ask their test partners to do the same!&lt;/p&gt;\n\n&lt;p&gt;It started off easy enough: I passed the initial &amp;#39;checks&amp;#39; as it confirmed my internet speed, camera access, and microphone access.  I started the process 15+ minutes before my scheduled exam time.  I was able to open the app, it again verified the technical requirements passed, and I went to the next screen.  It asked for my cell phone number and texted me a link which opened a web page which requested to take my photo.  Easy enough.  I did that and then the web page went to &amp;#39;Uploading and verifying photo&amp;#39;.  A spinning circle started spinning.  This is where my test experience ended, but not where the poor experience ends.  I tried again, and then a third time.  Same experience.  As I write this, I left it on that page and the spinning is continuing.  This screen has been spinning for no less than 45 minutes.  At 8 minutes before my scheduled exam, I tried finding the help link.  A chat window opened, and I waited, and waited, and waited.  Still waiting as I write this.  My chat window has been open for 52 minutes and still no one to help.  Every two minutes I get &amp;#39; All agents are currently assisting others. Thank you for your patience.&amp;#39; written in the window.  OK - what next?  They make it harder to find, but I got a phone number I can call.  I tried calling that.  Busy signal.  For the next 20 minutes I called back and back, busy signal.  Finally, I got it to actually pick up, but of course no human yet.  No estimate of time to when I can be helped.  They don&amp;#39;t even have nice elevator music to listen to.  Who knows when I will be able to talk to someone.  This has been an exceedingly poor experience.&lt;/p&gt;\n\n&lt;p&gt;If you value your time, please do yourself a favor and don&amp;#39;t even attempt a online exam with Pearson.  I worked hard to prepare for this exam and rescheduled things to fit around it.  Now, I will have to do that all again.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/u/jeffbarr\"&gt;u/jeffbarr&lt;/a&gt; Is this the experience AWS is hoping to get with their testing partners?  This was a waste of my time and money.  Amazon should seriously reevaluate the quality of their test partners.  I understand everyone is trying to deal with all the issues.  However, if you can&amp;#39;t offer quality testing, then please don&amp;#39;t offer the option at all.  It isn&amp;#39;t respectful to people&amp;#39;s time.  Pearson is well aware of their capacity and if it isn&amp;#39;t up to requirements, they shouldn&amp;#39;t be scheduling test slots.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;EDIT&lt;/em&gt;: A few background items I didn&amp;#39;t initially share that may be relevant for others.  For the computer, I used a fully up to date Windows 10 laptop.  The laptop itself is only about a month old and is in near pristine condition.  Other than a few applications like Office, there is barely anything installed on there yet.  I used a hard wired connection, like recommended by Pearson through the use of a usb-to-ethernet adapter.  I have Verizon FIOS (980Mbps/840Mbps) and did do a speed test way after it was apparent this would not work.  I forget the exact numbers, but I was still pulling in hundreds of Mbps in both directions, despite everyone being at home and using the USB ethernet adapater which does put a cap on my speed, but I can&amp;#39;t see hundreds of Mbps not being sufficent by orders of magnatude.  My phone is a fully up to date pixel 3.  I tried using my wifi in my house first (connected through FIOS), and then using the phone 4G LTE connection.  I can&amp;#39;t imagine this was caused by my end.  It seemed like Pearson&amp;#39;s servers were jammed at that point in time.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Update&lt;/em&gt;: After a LONG time, I did eventually get someone to answer from Pearson.  They were nice enough and were fairly easy to understand, although there was an delay echo introduced where whatever I said was echoed a quarter to half second later which was annoying, but bearable.  I was just happy she was able to hear me.  She said she could open a trouble ticket for me, but as it was well over an hour trying to get through to any human and doubtful it was on my side, I just told her to schedule me for the next available in person appointment.  She had to cancel my appointment and then rebook it as their sub-standard system wouldn&amp;#39;t let her reschedule an at home appointment to at a location.  Surprisingly, she said they would refund my money and rebook me.  It was painless enough, but when I asked for a reference number on the refund, all she could do is say I &amp;#39;should&amp;#39; get an email.  Perhaps unsurprisingly, this morning I see a fully posted charge for the rescheduled exam, but no sign of a refund.  Sigh.  I will give it a few days and then start this process over.&lt;/p&gt;\n\n&lt;p&gt;For what its worth, people should IGNORE the advice that the web chat is the fastest way of getting help.  Find the phone number and dial and re-dial it as fast as you can when you get a busy signal.  Despite the fact that it took 20+ minutes to get the number to pickup (and was &amp;#39;waiting&amp;#39; 20 minutes less from the phones point of view) I got a faster response from someone on the phone.  Web based chat never picked up, even though I left it running during my entire phone conversation.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Update #2&lt;/em&gt;: It took two more days than the charge, but the refund did show up in the correct amount on my credit card.  I am actually quite surprised.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed6858be-322a-11e9-a3f1-0e996dbdbce4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh84", "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "fscq7v", "is_robot_indexable": true, "report_reasons": null, "author": "VariousChallenge", "discussion_type": null, "num_comments": 96, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/aws/comments/fscq7v/psa_dont_take_remote_exams_offered_by_pearson_vue/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://www.reddit.com/r/aws/comments/fscq7v/psa_dont_take_remote_exams_offered_by_pearson_vue/", "subreddit_subscribers": 128616, "created_utc": 1585660596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "aws", "selftext": "I have a couple of aws credentials (Access + Security Key) and would like to know what are all the things it has access to.\n\nI do not have console access however. Just the keys and if there are any tools/scripts that can do it for me", "author_fullname": "t2_d62cqt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I check what access is available for a given Credentials programmatically?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/aws", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_hhx7kd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "discussion", "can_mod_post": false, "score": 9, "approved_by": null, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1593454049.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.aws", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a couple of aws credentials (Access + Security Key) and would like to know what are all the things it has access to.&lt;/p&gt;\n\n&lt;p&gt;I do not have console access however. Just the keys and if there are any tools/scripts that can do it for me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "962d796e-fa9c-11e8-a3dc-0e1ba4fe1be4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh84", "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "hhx7kd", "is_robot_indexable": true, "report_reasons": null, "author": "shiskeyoffles", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/aws/comments/hhx7kd/how_do_i_check_what_access_is_available_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/aws/comments/hhx7kd/how_do_i_check_what_access_is_available_for_a/", "subreddit_subscribers": 128616, "created_utc": 1593425249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "aws", "selftext": "Click 'Preferences' in the top right", "author_fullname": "t2_6h3jprr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Full dark mode (for both text + code samples) now available on the AWS documentation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/aws", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_hhgh82", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 127, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "general aws", "can_mod_post": false, "score": 127, "approved_by": null, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1593387487.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.aws", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Click &amp;#39;Preferences&amp;#39; in the top right&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "256aecca-fe52-11e8-bc65-0eea6867f0e4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh84", "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "hhgh82", "is_robot_indexable": true, "report_reasons": null, "author": "LittleJoeyHodges", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/aws/comments/hhgh82/full_dark_mode_for_both_text_code_samples_now/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/aws/comments/hhgh82/full_dark_mode_for_both_text_code_samples_now/", "subreddit_subscribers": 128616, "created_utc": 1593358687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "aws", "selftext": "Free tier is often insufficient to get a great idea off the ground without going into debt. If you had $10,000 in AWS credits, what would you build?", "author_fullname": "t2_5nwgcn4n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you had $10,000 in AWS credits, what would you build?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/aws", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_hhxti4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "discussion", "can_mod_post": false, "score": 1, "approved_by": null, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1593457247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.aws", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Free tier is often insufficient to get a great idea off the ground without going into debt. If you had $10,000 in AWS credits, what would you build?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "962d796e-fa9c-11e8-a3dc-0e1ba4fe1be4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh84", "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "hhxti4", "is_robot_indexable": true, "report_reasons": null, "author": "saaspiration", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/aws/comments/hhxti4/if_you_had_10000_in_aws_credits_what_would_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/aws/comments/hhxti4/if_you_had_10000_in_aws_credits_what_would_you/", "subreddit_subscribers": 128616, "created_utc": 1593428447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "aws", "selftext": "", "author_fullname": "t2_19ir6qfg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Power of Amazon Web Services", "link_flair_richtext": [], "subreddit_name_prefixed": "r/aws", "hidden": false, "pwls": 6, "link_flair_css_class": "article", "downs": 0, "top_awarded_type": null, "hide_score": true, "name": "t3_hhyljy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "article", "can_mod_post": false, "score": 0, "approved_by": null, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1593460963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "inflexguide.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.inflexguide.com/amazon-web-services-is-the-unbelievable-power-of-amazon/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "85ab6b1a-b9e3-11e6-847a-0e8ffa087616", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh84", "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "hhyljy", "is_robot_indexable": true, "report_reasons": null, "author": "Josephimrani", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/aws/comments/hhyljy/power_of_amazon_web_services/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.inflexguide.com/amazon-web-services-is-the-unbelievable-power-of-amazon/", "subreddit_subscribers": 128616, "created_utc": 1593432163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "aws", "selftext": "Has anyone tried to do backup of personal data using Amazon S3 - without a third party service like dropbox or [box.com](https://box.com) ?\n\n&amp;#x200B;\n\nwondering if that combined with Glacier can provide better value for secondary offsite backups for bulky data like years of personal photos and videos, or bulky media files generated when working with audio/video tools\n\n&amp;#x200B;\n\nare there any simple (open source) tools that can help us manage such backups, say schedule backups from home network from select drives, avoid duplicates, etc?", "author_fullname": "t2_coss6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal data backup using S3 buckets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/aws", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_hhev13", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "general aws", "can_mod_post": false, "score": 20, "approved_by": null, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1593381712.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.aws", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone tried to do backup of personal data using Amazon S3 - without a third party service like dropbox or &lt;a href=\"https://box.com\"&gt;box.com&lt;/a&gt; ?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;wondering if that combined with Glacier can provide better value for secondary offsite backups for bulky data like years of personal photos and videos, or bulky media files generated when working with audio/video tools&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;are there any simple (open source) tools that can help us manage such backups, say schedule backups from home network from select drives, avoid duplicates, etc?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "256aecca-fe52-11e8-bc65-0eea6867f0e4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh84", "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "hhev13", "is_robot_indexable": true, "report_reasons": null, "author": "a1b3rt", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/aws/comments/hhev13/personal_data_backup_using_s3_buckets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/aws/comments/hhev13/personal_data_backup_using_s3_buckets/", "subreddit_subscribers": 128616, "created_utc": 1593352912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "aws", "selftext": "Last month I connected my django project to AWS S3. I've configured my payment methods and the bill was around a few cents. The next month, my site was stripped off the database and I checked my bucket only to see it locked. I checked my payment and it says nothing is due. I have set the payment method to automatic and despite seeing that I have nothing due for the month, my account is locked. I've emailed them but have not gotten any replies. How do I go about this?", "author_fullname": "t2_4k6why66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Account Locked, No Help Gotten", "link_flair_richtext": [], "subreddit_name_prefixed": "r/aws", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_hhuksh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "storage", "can_mod_post": false, "score": 1, "approved_by": null, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1593439886.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.aws", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Last month I connected my django project to AWS S3. I&amp;#39;ve configured my payment methods and the bill was around a few cents. The next month, my site was stripped off the database and I checked my bucket only to see it locked. I checked my payment and it says nothing is due. I have set the payment method to automatic and despite seeing that I have nothing due for the month, my account is locked. I&amp;#39;ve emailed them but have not gotten any replies. How do I go about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "cb32bd78-fe51-11e8-8357-0e3adcef64d8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh84", "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "hhuksh", "is_robot_indexable": true, "report_reasons": null, "author": "worknovel", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/aws/comments/hhuksh/account_locked_no_help_gotten/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/aws/comments/hhuksh/account_locked_no_help_gotten/", "subreddit_subscribers": 128616, "created_utc": 1593411086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "aws", "selftext": "We want to store a snapshot of a resource configuration every 5 minutes in S3.\n\nThe key is unique (based on value hash and timestamp) and the value contains json value of the configuration.\n\nWe know that most users don't change their configuration 99% of times so 99% of data will be duplicate by the value. We are currently okay with this as we want to display configuration history to the users so they would know what their configuration was at any given timepoint. (How could you optimise that part so we wouldn't store duplicate data all the time?)\n\nAnyways, the S3 put object event triggers Lambda function that checks if resource configuration changed and if that is true, then does its magic, if not, then it does nothing.\n\nNow the thing is, how the heck would you know what the previous resource value was as S3 doesn't support indexing?\n\nSome options:\n1) Store processed value in redis and fetch it every time Lambda function is called and compare it to current value.\n2) Save metainfo (timestamp, and resource hash) into DynamoDB as well and query last resource hash with the latest timestamp to check if resource is different or not. Although I am not sure if DynamoDB is great fit for this, perhaps traditional RDS approach is better or maybe ElasticSearch?\n3) Use value hash as key instead, this means that each poller result is not unique anymore. So imagine a scenario, where:\nUser configures resource and the hash of json is 1234.\nUser changes a small detail and therefore the hash of json is 4321.\nUser reverts the change, and we are back to the hash of 1234.\nPoller keepings polling, and we get 1234 hash all the time, and this key value is all overwritten in S3 all the time.\nHowever, the Lambda function doesn't know what the previous hash was, so this is always processed.\n\nWhat other options are there? How would you solve this?\n\nTL;DR: a resource configuration json is polled every 5 minutes and saved to S3, most of the times it does not change. How to make Lambda function, that gets triggered by S3 upload, aware what the previous hash was so that it would not do anything if the resource configuration did not change?", "author_fullname": "t2_hfd4j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Architecting S3 and Lambda for configuration snapshots that do not change that often?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/aws", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_hhn4n6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "discussion", "can_mod_post": false, "score": 5, "approved_by": null, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1593409469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.aws", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We want to store a snapshot of a resource configuration every 5 minutes in S3.&lt;/p&gt;\n\n&lt;p&gt;The key is unique (based on value hash and timestamp) and the value contains json value of the configuration.&lt;/p&gt;\n\n&lt;p&gt;We know that most users don&amp;#39;t change their configuration 99% of times so 99% of data will be duplicate by the value. We are currently okay with this as we want to display configuration history to the users so they would know what their configuration was at any given timepoint. (How could you optimise that part so we wouldn&amp;#39;t store duplicate data all the time?)&lt;/p&gt;\n\n&lt;p&gt;Anyways, the S3 put object event triggers Lambda function that checks if resource configuration changed and if that is true, then does its magic, if not, then it does nothing.&lt;/p&gt;\n\n&lt;p&gt;Now the thing is, how the heck would you know what the previous resource value was as S3 doesn&amp;#39;t support indexing?&lt;/p&gt;\n\n&lt;p&gt;Some options:\n1) Store processed value in redis and fetch it every time Lambda function is called and compare it to current value.\n2) Save metainfo (timestamp, and resource hash) into DynamoDB as well and query last resource hash with the latest timestamp to check if resource is different or not. Although I am not sure if DynamoDB is great fit for this, perhaps traditional RDS approach is better or maybe ElasticSearch?\n3) Use value hash as key instead, this means that each poller result is not unique anymore. So imagine a scenario, where:\nUser configures resource and the hash of json is 1234.\nUser changes a small detail and therefore the hash of json is 4321.\nUser reverts the change, and we are back to the hash of 1234.\nPoller keepings polling, and we get 1234 hash all the time, and this key value is all overwritten in S3 all the time.\nHowever, the Lambda function doesn&amp;#39;t know what the previous hash was, so this is always processed.&lt;/p&gt;\n\n&lt;p&gt;What other options are there? How would you solve this?&lt;/p&gt;\n\n&lt;p&gt;TL;DR: a resource configuration json is polled every 5 minutes and saved to S3, most of the times it does not change. How to make Lambda function, that gets triggered by S3 upload, aware what the previous hash was so that it would not do anything if the resource configuration did not change?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "962d796e-fa9c-11e8-a3dc-0e1ba4fe1be4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh84", "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "hhn4n6", "is_robot_indexable": true, "report_reasons": null, "author": "IamJustAWizard", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/aws/comments/hhn4n6/architecting_s3_and_lambda_for_configuration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/aws/comments/hhn4n6/architecting_s3_and_lambda_for_configuration/", "subreddit_subscribers": 128616, "created_utc": 1593380669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "aws", "selftext": "When using a DyanamoDB global table that is replicated to multiple regions there does not seem to be an easy way in Lambda@Edge to determine what is the closest AWS region to make API calls against.\n\nIt seems the way to do this would be to setup multiple CloudFront distributions, then have Route53 perform latency based routing to those CF distributions.  Finally in each CF distribution the Lambda@Edge function would map the CF DistributionId back to the particular AWS region as specified in the Route53 latency record.\n\nIs there a better approach that I'm missing?", "author_fullname": "t2_4wqzgkrz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DynamoDB Global Table with Lambda@Edge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/aws", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_hht7dg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "technical question", "can_mod_post": false, "score": 0, "approved_by": null, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1593433370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.aws", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When using a DyanamoDB global table that is replicated to multiple regions there does not seem to be an easy way in Lambda@Edge to determine what is the closest AWS region to make API calls against.&lt;/p&gt;\n\n&lt;p&gt;It seems the way to do this would be to setup multiple CloudFront distributions, then have Route53 perform latency based routing to those CF distributions.  Finally in each CF distribution the Lambda@Edge function would map the CF DistributionId back to the particular AWS region as specified in the Route53 latency record.&lt;/p&gt;\n\n&lt;p&gt;Is there a better approach that I&amp;#39;m missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "e0acaab0-fe51-11e8-b457-0e86fa5111f4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh84", "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "hht7dg", "is_robot_indexable": true, "report_reasons": null, "author": "fepluso", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/aws/comments/hht7dg/dynamodb_global_table_with_lambdaedge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/aws/comments/hht7dg/dynamodb_global_table_with_lambdaedge/", "subreddit_subscribers": 128616, "created_utc": 1593404570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "aws", "selftext": "I need to use an existing API to get access to upload a file to s3. The api should grant me enough info to upload a file to their s3 bucket. The API gives me the following info:\n\n1. a region\n2. s3 bucket name\n3. cognito identity id us-east-1:aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\n4. A JWT Token (valid for a very short period of time)\n\nI'm not entirely sure what I can do with the jwt? When I decode it. it has an `amr\":[\"authenticated\",\"my-app\",\"my-app:us-east-1:&lt;id&gt;:&lt;user id&gt;\"` part to it. Can anyone tell me what this jwt is exactly?\n\nI'd like to use python to do the uploading, but not sure how this works. Can anyone point me in the right direction?", "author_fullname": "t2_ccz3j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to upload a file to s3 with Cognito auth", "link_flair_richtext": [], "subreddit_name_prefixed": "r/aws", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_hhp8ph", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "technical question", "can_mod_post": false, "score": 2, "approved_by": null, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1593417196.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.aws", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to use an existing API to get access to upload a file to s3. The api should grant me enough info to upload a file to their s3 bucket. The API gives me the following info:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;a region&lt;/li&gt;\n&lt;li&gt;s3 bucket name&lt;/li&gt;\n&lt;li&gt;cognito identity id us-east-1:aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa&lt;/li&gt;\n&lt;li&gt;A JWT Token (valid for a very short period of time)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m not entirely sure what I can do with the jwt? When I decode it. it has an &lt;code&gt;amr&amp;quot;:[&amp;quot;authenticated&amp;quot;,&amp;quot;my-app&amp;quot;,&amp;quot;my-app:us-east-1:&amp;lt;id&amp;gt;:&amp;lt;user id&amp;gt;&amp;quot;&lt;/code&gt; part to it. Can anyone tell me what this jwt is exactly?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to use python to do the uploading, but not sure how this works. Can anyone point me in the right direction?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "e0acaab0-fe51-11e8-b457-0e86fa5111f4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh84", "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "hhp8ph", "is_robot_indexable": true, "report_reasons": null, "author": "man_with_cat2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/aws/comments/hhp8ph/how_to_upload_a_file_to_s3_with_cognito_auth/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/aws/comments/hhp8ph/how_to_upload_a_file_to_s3_with_cognito_auth/", "subreddit_subscribers": 128616, "created_utc": 1593388396.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "aws", "selftext": "I am on 30GB free tier in my Windows ec2, sometimes I needed 80GB of storage for 2 hours, how do I increase the storage and revert it to 30GB free tier in a cost efficient way?", "author_fullname": "t2_1t2nedvl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Temporarily increasing hard disk storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/aws", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_hhs3ew", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "technical question", "can_mod_post": false, "score": 1, "approved_by": null, "author_premium": false, "thumbnail": "", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1593428546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.aws", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am on 30GB free tier in my Windows ec2, sometimes I needed 80GB of storage for 2 hours, how do I increase the storage and revert it to 30GB free tier in a cost efficient way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "e0acaab0-fe51-11e8-b457-0e86fa5111f4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh84", "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "hhs3ew", "is_robot_indexable": true, "report_reasons": null, "author": "cinemonloops", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/aws/comments/hhs3ew/temporarily_increasing_hard_disk_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/aws/comments/hhs3ew/temporarily_increasing_hard_disk_storage/", "subreddit_subscribers": 128616, "created_utc": 1593399746.0, "num_crossposts": 0, "media": null, "is_video": false}}], "after": "t3_hhs3ew", "before": null}}